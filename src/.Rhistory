MyData_test = MyData_t
predictions = predict(tree_ms10, MyData_test)
vec=NULL
for (i in 1:dim(predictions)[1])
{
vec[i]=names(which(predictions[i,]==max(predictions[i,])))
}
print("Séquence prédites:")
print(vec)
print("Vérification de la validité du modèle en l'alignant sur nos données de test.")
print(table(vec,MyData_t$AA))
dev.off()
MyData_t <- read.csv(file="../result/test/vecteur_vecteur_to_predict.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData_t <- read.csv(file="../result/test/vecteur_to_predict.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
library("rpart")
library("rpart.plot")
#Récupération des arguments
variable <- commandArgs(trailingOnly=TRUE)
#lecture du fichier csv
#MyData <- read.csv(file=variable[1], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData <- read.csv(file="../result/test/vecteur_db.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData[is.na(MyData)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData$ACC<-(MyData$ACC-mean(MyData$ACC))/sd(MyData$ACC)
MyData$polar<-(MyData$polar-mean(MyData$polar))/sd(MyData$polar)
MyData$hydrogenB<-(MyData$hydrogenB-mean(MyData$hydrogenB))/sd(MyData$hydrogenB)
MyData$hydrophobic<-(MyData$hydrophobic-mean(MyData$hydrophobic))/sd(MyData$hydrophobic)
MyData$vdw<-(MyData$vdw-mean(MyData$vdw))/sd(MyData$vdw)
MyData$ionic<-(MyData$ionic-mean(MyData$ionic))/sd(MyData$ionic)
#Constitution groupe d'apprentissage et de test
MyData_train = MyData
#Construction de l'arbre de décision
target= AA ~SS+ACC+a_PB+b_PB+c_PB+d_PB+e_PB+f_PB+g_PB+h_PB+i_PB+j_PB+k_PB+l_PB+m_PB+n_PB+o_PB+p_PB+polar+ionic+hydrophobic+vdw+hydrogenB
a="../result/test/"
#a=variable[3]
pdf(paste(a,"predict.pdf"))
fq_att=table(MyData$AA)
vecteur=c(1/fq_att)
w = NULL#vecteur de poids depend des fréquences il doit mesurer 110
for (i in 1:dim(MyData_train)[1])
{
w[i]=vecteur[which(names(vecteur)==MyData_train$AA[i])]
}
mytree <- rpart(target, data = MyData_train, method = "class", minsplit = 2, minbucket = 1, weights = w)
#minsplit est "le nombre minimum d'observations qui doivent exister dans un noeud pour qu'une tentative de division soit effectuée" et minbucket est "le nombre minimum d'observations dans un noeud terminal"
rpart.plot(mytree,main='Arbre de décision')
#Elagage
tree_ms3 = rpart(target, MyData_train, control = rpart.control(minsplit = 3))
tree_ms10 = rpart(target, MyData_train, control = rpart.control(minsplit = 10))
rpart.plot(tree_ms3, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) = 3")
rpart.plot(tree_ms10, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) =10")
#prediction sur le jeu test
#lecture du fichier csv
#MyData_t<- read.csv(file=variable[2], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData_t <- read.csv(file="../result/test/vecteur_to_predict.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData_t[is.na(MyData_t)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData_t$ACC<-(MyData_t$ACC-mean(MyData_t$ACC))/sd(MyData_t$ACC)
MyData_t$polar<-(MyData_t$polar-mean(MyData_t$polar))/sd(MyData_t$polar)
MyData_t$hydrogenB<-(MyData_t$hydrogenB-mean(MyData_t$hydrogenB))/sd(MyData_t$hydrogenB)
MyData_t$hydrophobic<-(MyData_t$hydrophobic-mean(MyData_t$hydrophobic))/sd(MyData_t$hydrophobic)
MyData_t$vdw<-(MyData_t$vdw-mean(MyData_t$vdw))/sd(MyData_t$vdw)
MyData_t$ionic<-(MyData_t$ionic-mean(MyData_t$ionic))/sd(MyData_t$ionic)
MyData_test = MyData_t
predictions = predict(tree_ms10, MyData_test)
vec=NULL
for (i in 1:dim(predictions)[1])
{
vec[i]=names(which(predictions[i,]==max(predictions[i,])))
}
print("Séquence prédites:")
print(vec)
print("Vérification de la validité du modèle en l'alignant sur nos données de test.")
print(table(vec,MyData_t$AA))
dev.off()
View(MyData_t)
View(MyData_t)
View(MyData_train)
View(MyData_train)
mytree <- rpart(target, data = MyData_train, method = "class", minsplit = 2, minbucket = 1, weights = w)
mytree
rpart(target, data = MyData_train, method = "class", minsplit = 2, minbucket = 1, weights = w)
summary(MyData_train)
target= AA ~H_ss+B_ss+E_ss+G_ss+I_ss+T_ss+S_ss+-_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+ACC+a_PB+b_PB+c_PB+d_PB+e_PB+f_PB+g_PB+h_PB+i_PB+j_PB+k_PB+l_PB+m_PB+n_PB+o_PB+p_PB+polar+ionic+hydrophobic+vdw+hydrogenB
target= AA ~H_ss+B_ss+E_ss+G_ss+I_ss+T_ss+S_ss+X._ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+ACC+a_PB+b_PB+c_PB+d_PB+e_PB+f_PB+g_PB+h_PB+i_PB+j_PB+k_PB+l_PB+m_PB+n_PB+o_PB+p_PB+polar+ionic+hydrophobic+vdw+hydrogenB
a="../result/test/"
fq_att=table(MyData$AA)
vecteur=c(1/fq_att)
w = NULL#vecteur de poids depend des fréquences il doit mesurer 110
for (i in 1:dim(MyData_train)[1])
{
w[i]=vecteur[which(names(vecteur)==MyData_train$AA[i])]
}
mytree <- rpart(target, data = MyData_train, method = "class", minsplit = 2, minbucket = 1, weights = w)
rpart.plot(mytree,main='Arbre de décision')
tree_ms3 = rpart(target, MyData_train, control = rpart.control(minsplit = 3))
tree_ms10 = rpart(target, MyData_train, control = rpart.control(minsplit = 10))
rpart.plot(tree_ms3, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) = 3")
rpart.plot(tree_ms10, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) =10")
rpart.plot(mytree,main='Arbre de décision')
#!/usr/bin/Rscript --slave
#Packages
# install.packages("rpart")
# install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
#Récupération des arguments
variable <- commandArgs(trailingOnly=TRUE)
#lecture du fichier csv
#MyData <- read.csv(file=variable[1], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData <- read.csv(file="../result/test/vecteur_db.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData[is.na(MyData)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData$ACC<-(MyData$ACC-mean(MyData$ACC))/sd(MyData$ACC)
MyData$polar<-(MyData$polar-mean(MyData$polar))/sd(MyData$polar)
MyData$hydrogenB<-(MyData$hydrogenB-mean(MyData$hydrogenB))/sd(MyData$hydrogenB)
MyData$hydrophobic<-(MyData$hydrophobic-mean(MyData$hydrophobic))/sd(MyData$hydrophobic)
MyData$vdw<-(MyData$vdw-mean(MyData$vdw))/sd(MyData$vdw)
MyData$ionic<-(MyData$ionic-mean(MyData$ionic))/sd(MyData$ionic)
#Constitution groupe d'apprentissage et de test
MyData_train = MyData
#Construction de l'arbre de décision
target= AA ~H_ss+B_ss+E_ss+G_ss+I_ss+T_ss+S_ss+X._ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+ACC+a_PB+b_PB+c_PB+d_PB+e_PB+f_PB+g_PB+h_PB+i_PB+j_PB+k_PB+l_PB+m_PB+n_PB+o_PB+p_PB+polar+ionic+hydrophobic+vdw+hydrogenB
a="../result/test/"
#a=variable[3]
pdf(paste(a,"predict.pdf"))
fq_att=table(MyData$AA)
vecteur=c(1/fq_att)
w = NULL#vecteur de poids depend des fréquences il doit mesurer 110
for (i in 1:dim(MyData_train)[1])
{
w[i]=vecteur[which(names(vecteur)==MyData_train$AA[i])]
}
mytree <- rpart(target, data = MyData_train, method = "class", minsplit = 2, minbucket = 1, weights = w)
#minsplit est "le nombre minimum d'observations qui doivent exister dans un noeud pour qu'une tentative de division soit effectuée" et minbucket est "le nombre minimum d'observations dans un noeud terminal"
rpart.plot(mytree,main='Arbre de décision')
#Elagage
tree_ms3 = rpart(target, MyData_train, control = rpart.control(minsplit = 3))
tree_ms10 = rpart(target, MyData_train, control = rpart.control(minsplit = 10))
rpart.plot(tree_ms3, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) = 3")
rpart.plot(tree_ms10, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) =10")
MyData_t <- read.csv(file="../result/test/vecteur_to_predict.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData_t[is.na(MyData_t)] <- 0 #remplace les NA (s'il y en as) par des 0
MyData_t$ACC<-(MyData_t$ACC-mean(MyData_t$ACC))/sd(MyData_t$ACC)
MyData_t$polar<-(MyData_t$polar-mean(MyData_t$polar))/sd(MyData_t$polar)
MyData_t$hydrogenB<-(MyData_t$hydrogenB-mean(MyData_t$hydrogenB))/sd(MyData_t$hydrogenB)
MyData_t$hydrophobic<-(MyData_t$hydrophobic-mean(MyData_t$hydrophobic))/sd(MyData_t$hydrophobic)
MyData_t$vdw<-(MyData_t$vdw-mean(MyData_t$vdw))/sd(MyData_t$vdw)
MyData_t$ionic<-(MyData_t$ionic-mean(MyData_t$ionic))/sd(MyData_t$ionic)
MyData_test = MyData_t
predictions = predict(tree_ms10, MyData_test)
vec=NULL
for (i in 1:dim(predictions)[1])
{
vec[i]=names(which(predictions[i,]==max(predictions[i,])))
}
print("Séquence prédites:")
print(vec)
print("Vérification de la validité du modèle en l'alignant sur nos données de test.")
print(table(vec,MyData_t$AA))
dev.off()
#!/usr/bin/Rscript --slave
#Packages
# install.packages("rpart")
# install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
#Récupération des arguments
variable <- commandArgs(trailingOnly=TRUE)
#lecture du fichier csv
#MyData <- read.csv(file=variable[1], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData <- read.csv(file="../result/test/vecteur_db.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData[is.na(MyData)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData$ACC<-(MyData$ACC-mean(MyData$ACC))/sd(MyData$ACC)
MyData$polar<-(MyData$polar-mean(MyData$polar))/sd(MyData$polar)
MyData$hydrogenB<-(MyData$hydrogenB-mean(MyData$hydrogenB))/sd(MyData$hydrogenB)
MyData$hydrophobic<-(MyData$hydrophobic-mean(MyData$hydrophobic))/sd(MyData$hydrophobic)
MyData$vdw<-(MyData$vdw-mean(MyData$vdw))/sd(MyData$vdw)
MyData$ionic<-(MyData$ionic-mean(MyData$ionic))/sd(MyData$ionic)
#Constitution groupe d'apprentissage et de test
MyData_train = MyData
#Construction de l'arbre de décision
target= AA ~H_ss+B_ss+E_ss+G_ss+I_ss+T_ss+S_ss+X._ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+o_ss+ACC+a_PB+b_PB+c_PB+d_PB+e_PB+f_PB+g_PB+h_PB+i_PB+j_PB+k_PB+l_PB+m_PB+n_PB+o_PB+p_PB+polar+ionic+hydrophobic+vdw+hydrogenB
a="../result/test/"
#a=variable[3]
pdf(paste(a,"predict.pdf"))
fq_att=table(MyData$AA)
vecteur=c(1/fq_att)
w = NULL#vecteur de poids depend des fréquences il doit mesurer 110
for (i in 1:dim(MyData_train)[1])
{
w[i]=vecteur[which(names(vecteur)==MyData_train$AA[i])]
}
mytree <- rpart(target, data = MyData_train, method = "class", minsplit = 2, minbucket = 1, weights = w)
#minsplit est "le nombre minimum d'observations qui doivent exister dans un noeud pour qu'une tentative de division soit effectuée" et minbucket est "le nombre minimum d'observations dans un noeud terminal"
rpart.plot(mytree,main='Arbre de décision')
#Elagage
tree_ms3 = rpart(target, MyData_train, control = rpart.control(minsplit = 3))
tree_ms10 = rpart(target, MyData_train, control = rpart.control(minsplit = 10))
rpart.plot(tree_ms3, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) = 3")
rpart.plot(tree_ms10, main = "Prunning: minsplit(nombre minimum d'observations dans un noeud) =10")
#prediction sur le jeu test
#lecture du fichier csv
#MyData_t<- read.csv(file=variable[2], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData_t <- read.csv(file="../result/test/vecteur_to_predict.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData_t[is.na(MyData_t)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData_t$ACC<-(MyData_t$ACC-mean(MyData_t$ACC))/sd(MyData_t$ACC)
MyData_t$polar<-(MyData_t$polar-mean(MyData_t$polar))/sd(MyData_t$polar)
MyData_t$hydrogenB<-(MyData_t$hydrogenB-mean(MyData_t$hydrogenB))/sd(MyData_t$hydrogenB)
MyData_t$hydrophobic<-(MyData_t$hydrophobic-mean(MyData_t$hydrophobic))/sd(MyData_t$hydrophobic)
MyData_t$vdw<-(MyData_t$vdw-mean(MyData_t$vdw))/sd(MyData_t$vdw)
MyData_t$ionic<-(MyData_t$ionic-mean(MyData_t$ionic))/sd(MyData_t$ionic)
MyData_test = MyData_t
predictions = predict(mytree , MyData_test)
vec=NULL
for (i in 1:dim(predictions)[1])
{
vec[i]=names(which(predictions[i,]==max(predictions[i,])))
}
print("Séquence prédites:")
print(vec)
print("Vérification de la validité du modèle en l'alignant sur nos données de test.")
print(table(vec,MyData_t$AA))
dev.off()
barplot(mat[,1])
mat[,1]
axis(4,at=seq(0,800,200),labels=seq(0,800,200)/1000)
barplot(mat[,1])
mat <- matrix(nrow=length(nb_att), ncol=nb_cluster)
for (i in 1:length(nb_att))
{
for(j in 1:nb_cluster){
a=log2(fq_obs[i,j]/fq_att[i])
mat[i,j]=ifelse(a == -Inf,0,a)
}
}
mat <- matrix(nrow=length(nb_att), ncol=nb_cluster)
for (i in 1:length(nb_att))
{
for(j in 1:nb_cluster){
a=log2(fq_obs[i,j]/fq_att[i])
mat[i,j]=ifelse(a == -Inf,0,a)
}
}
for (i in 1:dim(mat)[2])
{
barplot(mat[,i],main='Distribution des scores (Méthodes Kmeans)',xlab=paste("Cluster ",i),ylab="Score",ylim=c(0,-10))
}
#Packages
install.packages("ggfortify")
library("ggplot2")
library("ggfortify")
install.packages("tidyverse")
library(tidyverse)  # data manipulation
install.packages("factoextra")
library(factoextra) # clustering algorithms & visualization
library(cluster)
#Récupération des arguments
variable <- commandArgs(trailingOnly=TRUE)
#lecture du fichier csv
MyData <- read.csv(file="../result/test/vecteur_db.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
#MyData <- read.csv(file=variable[1], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData[is.na(MyData)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData$ACC<-(MyData$ACC-mean(MyData$ACC))/sd(MyData$ACC)
MyData$polar<-(MyData$polar-mean(MyData$polar))/sd(MyData$polar)
MyData$hydrogenB<-(MyData$hydrogenB-mean(MyData$hydrogenB))/sd(MyData$hydrogenB)
MyData$hydrophobic<-(MyData$hydrophobic-mean(MyData$hydrophobic))/sd(MyData$hydrophobic)
MyData$vdw<-(MyData$vdw-mean(MyData$vdw))/sd(MyData$vdw)
MyData$ionic<-(MyData$ionic-mean(MyData$ionic))/sd(MyData$ionic)
#PCA
#center = TRUE,scale. = TRUE
MyData.Vector <- MyData[, 2:length(MyData)]
MyData.AA <- MyData$AA
MyData.Vector.pca <- prcomp(MyData.Vector)
a="result/test/"
#a=variable[2]
pdf("../result/test/cluster.pdf")
plot(MyData.Vector.pca, type = "l",main="Variances en fonction des Composantes Principales")
summary(MyData.Vector.pca)
# Plot
# Colorer en fonction du cos2: qualité de représentation
fviz_pca_var(MyData.Vector.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Évite le chevauchement de texte
)
#clusterisation
#kmeans
#DETERMINATION NB CLUSTER
nb_cluster=5
fviz_nbclust(MyData.Vector, kmeans, method = "wss")+
geom_vline(xintercept = nb_cluster, linetype = 2)
#CALCUL
set.seed(8953)
MyData2 <- MyData
MyData2$AA <- NULL
(kmeans.result <- kmeans(MyData2, nb_cluster))
#plot
autoplot(kmeans.result, data = MyData, label = TRUE, label.size = nb_cluster, frame = TRUE,main="Clustering par Kmeans")
#calcul fréquences
#score=log2(fqobs/fqatt)
nb_obs=table(MyData$AA, kmeans.result$cluster)
fq_obs=round(prop.table(nb_obs), nb_cluster)#fréquence
nb_att=table(MyData$AA)
fq_att=round(prop.table(nb_att), nb_cluster)#fréquence
#matrice score par cluster
mat <- matrix(nrow=length(nb_att), ncol=nb_cluster)
for (i in 1:length(nb_att))
{
for(j in 1:nb_cluster){
a=log2(fq_obs[i,j]/fq_att[i])
mat[i,j]=ifelse(a == -Inf,0,a)
}
}
colnames(mat)=colnames(nb_obs)
rownames(mat)=rownames(nb_obs)
print("Matrice des scores par cluster:")
print(mat)
#par(mfrow=c(2,dim(mat)[2]/2))
for (i in 1:dim(mat)[2])
{
barplot(mat[,i],main='Distribution des scores (Méthodes Kmeans)',xlab=paste("Cluster ",i),ylab="Score",ylim=c(0,-10))
}
#kmedoid
#Clustering with pam()
pam.res=pam(MyData.Vector, nb_cluster)
autoplot(pam.res, frame = TRUE, frame.type = 'norm',main="Clustering par Kmedoid(PAM)")
fviz_silhouette(silhouette(pam.res))
# Compute silhouette
sil <- silhouette(pam.res)[, 1:3]
# Objects with negative silhouette
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
#calcul fréquences
#log2(fqobs/fqatt)=score
nb_obs=table(MyData$AA, pam.res$cluster)
fq_obs=round(prop.table(nb_obs), nb_cluster)#fréquence
nb_att=table(MyData$AA)
fq_att=round(prop.table(nb_att), nb_cluster)#fréquence
#matrice score par cluster
mat <- matrix(nrow=length(nb_att), ncol=nb_cluster)
for (i in 1:length(nb_att))
{
for(j in 1:nb_cluster){
a=log2(fq_obs[i,j]/fq_att[i])
mat[i,j]=ifelse(a == -Inf,0,a)
}
}
colnames(mat)=colnames(nb_obs)
rownames(mat)=rownames(nb_obs)
print("Matrice des scores par cluster:")
print(mat)
par(mfrow=c(2,dim(mat)[2]/2))
for (i in 1:dim(mat)[2])
{
barplot(mat[,i],main='Distribution des scores (Méthode PAM)',xlab=paste("Cluster ",i),ylab="Score",ylim=c(0,-10))
}
dev.off()
install.packages("ggfortify")
install.packages("factoextra")
install.packages("factoextra")
#Packages
install.packages("ggfortify")
library("ggplot2")
library("ggfortify")
install.packages("tidyverse")
library(tidyverse)  # data manipulation
install.packages("factoextra")
library(factoextra) # clustering algorithms & visualization
library(cluster)
#Récupération des arguments
variable <- commandArgs(trailingOnly=TRUE)
#lecture du fichier csv
MyData <- read.csv(file="../result/test/vecteur_db.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
#MyData <- read.csv(file=variable[1], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData[is.na(MyData)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData$ACC<-(MyData$ACC-mean(MyData$ACC))/sd(MyData$ACC)
MyData$polar<-(MyData$polar-mean(MyData$polar))/sd(MyData$polar)
MyData$hydrogenB<-(MyData$hydrogenB-mean(MyData$hydrogenB))/sd(MyData$hydrogenB)
MyData$hydrophobic<-(MyData$hydrophobic-mean(MyData$hydrophobic))/sd(MyData$hydrophobic)
MyData$vdw<-(MyData$vdw-mean(MyData$vdw))/sd(MyData$vdw)
MyData$ionic<-(MyData$ionic-mean(MyData$ionic))/sd(MyData$ionic)
#PCA
#center = TRUE,scale. = TRUE
MyData.Vector <- MyData[, 2:length(MyData)]
MyData.AA <- MyData$AA
MyData.Vector.pca <- prcomp(MyData.Vector)
a="result/test/"
#a=variable[2]
pdf("../result/test/cluster.pdf")
plot(MyData.Vector.pca, type = "l",main="Variances en fonction des Composantes Principales")
summary(MyData.Vector.pca)
# Plot
# Colorer en fonction du cos2: qualité de représentation
fviz_pca_var(MyData.Vector.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Évite le chevauchement de texte
)
#clusterisation
#kmeans
#DETERMINATION NB CLUSTER
nb_cluster=5
fviz_nbclust(MyData.Vector, kmeans, method = "wss")+
geom_vline(xintercept = nb_cluster, linetype = 2)
#CALCUL
set.seed(8953)
MyData2 <- MyData
MyData2$AA <- NULL
(kmeans.result <- kmeans(MyData2, nb_cluster))
#plot
autoplot(kmeans.result, data = MyData, label = TRUE, label.size = nb_cluster, frame = TRUE,main="Clustering par Kmeans")
#calcul fréquences
#score=log2(fqobs/fqatt)
nb_obs=table(MyData$AA, kmeans.result$cluster)
fq_obs=round(prop.table(nb_obs), nb_cluster)#fréquence
nb_att=table(MyData$AA)
fq_att=round(prop.table(nb_att), nb_cluster)#fréquence
#matrice score par cluster
mat <- matrix(nrow=length(nb_att), ncol=nb_cluster)
for (i in 1:length(nb_att))
{
for(j in 1:nb_cluster){
a=log2(fq_obs[i,j]/fq_att[i])
mat[i,j]=ifelse(a == -Inf,0,a)
}
}
colnames(mat)=colnames(nb_obs)
rownames(mat)=rownames(nb_obs)
print("Matrice des scores par cluster:")
print(mat)
#par(mfrow=c(2,dim(mat)[2]/2))
for (i in 1:dim(mat)[2])
{
barplot(mat[,i],main='Distribution des scores (Méthodes Kmeans)',xlab=paste("Cluster ",i),ylab="Score",ylim=c(0,-10))
}
#kmedoid
#Clustering with pam()
pam.res=pam(MyData.Vector, nb_cluster)
autoplot(pam.res, frame = TRUE, frame.type = 'norm',main="Clustering par Kmedoid(PAM)")
fviz_silhouette(silhouette(pam.res))
# Compute silhouette
sil <- silhouette(pam.res)[, 1:3]
# Objects with negative silhouette
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
#calcul fréquences
#log2(fqobs/fqatt)=score
nb_obs=table(MyData$AA, pam.res$cluster)
fq_obs=round(prop.table(nb_obs), nb_cluster)#fréquence
nb_att=table(MyData$AA)
fq_att=round(prop.table(nb_att), nb_cluster)#fréquence
#matrice score par cluster
mat <- matrix(nrow=length(nb_att), ncol=nb_cluster)
for (i in 1:length(nb_att))
{
for(j in 1:nb_cluster){
a=log2(fq_obs[i,j]/fq_att[i])
mat[i,j]=ifelse(a == -Inf,0,a)
}
}
colnames(mat)=colnames(nb_obs)
rownames(mat)=rownames(nb_obs)
print("Matrice des scores par cluster:")
print(mat)
#par(mfrow=c(2,dim(mat)[2]/2))
for (i in 1:dim(mat)[2])
{
barplot(mat[,i],main='Distribution des scores (Méthode PAM)',xlab=paste("Cluster ",i),ylab="Score",ylim=c(0,-10))
}
dev.off()
install.packages("ggfortify")
install.packages("factoextra")
install.packages("factoextra")
MyData_t <- read.csv(file="../result/test/vecteur_to_predict.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
library("rpart")
library("ggplot2")
install.packages("caret",
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
zero.test
zero.test()
library(ggplot2)
library(ggfortify)
#install.packages("factoextra")
library(factoextra) # clustering algorithms & visualization
library(cluster)
#kohonen library
#install.packages("kohonen")
library(kohonen)
#install.packages(ProjectionBasedClustering)
library(ProjectionBasedClustering)
MyData <- read.csv(file="~/Bureau/projet_long/result/other/vecteur_db.csv", header=TRUE, sep=";",stringsAsFactors=FALSE)
#MyData <- read.csv(file=variable[1], header=TRUE, sep=";",stringsAsFactors=FALSE)
MyData[is.na(MyData)] <- 0 #remplace les NA (s'il y en as) par des 0
#normalisation par zscore
MyData$ACC<-(MyData$ACC-mean(MyData$ACC))/sd(MyData$ACC)
MyData$polar<-(MyData$polar-mean(MyData$polar))/sd(MyData$polar)
MyData$hydrogenB<-(MyData$hydrogenB-mean(MyData$hydrogenB))/sd(MyData$hydrogenB)
MyData$hydrophobic<-(MyData$hydrophobic-mean(MyData$hydrophobic))/sd(MyData$hydrophobic)
MyData$vdw<-(MyData$vdw-mean(MyData$vdw))/sd(MyData$vdw)
MyData$ionic<-(MyData$ionic-mean(MyData$ionic))/sd(MyData$ionic)
##############################################################PCA
################################""ACP
MyData.Vector <- MyData[, 2:length(MyData)]
MyData.AA <- MyData$AA
MyData.Vector.pca <- prcomp(MyData.Vector)
fviz_eig(MyData.Vector.pca,addlabels=TRUE,ylim=c(0,50))
fviz_eig(MyData.Vector.pca,addlabels=TRUE,ylim=c(0,50))
33.6+13.7
